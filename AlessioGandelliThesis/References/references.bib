
@misc{hoyle_is_2021,
	title = {Is {Automated} {Topic} {Model} {Evaluation} {Broken}?: {The} {Incoherence} of {Coherence}},
	shorttitle = {Is {Automated} {Topic} {Model} {Evaluation} {Broken}?},
	url = {http://arxiv.org/abs/2107.02173},
	doi = {10.48550/arXiv.2107.02173},
	abstract = {Topic model evaluation, like evaluation of other unsupervised methods, can be contentious. However, the field has coalesced around automated estimates of topic coherence, which rely on the frequency of word co-occurrences in a reference corpus. Contemporary neural topic models surpass classical ones according to these metrics. At the same time, topic model evaluation suffers from a validation gap: automated coherence, developed for classical models, has not been validated using human experimentation for neural models. In addition, a meta-analysis of topic modeling literature reveals a substantial standardization gap in automated topic modeling benchmarks. To address the validation gap, we compare automated coherence with the two most widely accepted human judgment tasks: topic rating and word intrusion. To address the standardization gap, we systematically evaluate a dominant classical model and two state-of-the-art neural models on two commonly used datasets. Automated evaluations declare a winning model when corresponding human evaluations do not, calling into question the validity of fully automatic evaluations independent of human judgments.},
	urldate = {2023-03-29},
	publisher = {arXiv},
	author = {Hoyle, Alexander and Goel, Pranav and Peskov, Denis and Hian-Cheong, Andrew and Boyd-Graber, Jordan and Resnik, Philip},
	month = oct,
	year = {2021},
	note = {arXiv:2107.02173 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{lee_algorithms_2000,
	title = {Algorithms for {Non}-negative {Matrix} {Factorization}},
	volume = {13},
	url = {https://papers.nips.cc/paper/2000/hash/f9d1152547c0bde01830b7e8bd60024c-Abstract.html},
	abstract = {Non-negative matrix factorization (NMF) has previously been shown to 
be a useful decomposition for multivariate data. Two different multi- 
plicative algorithms for NMF are analyzed. They differ only slightly in 
the multiplicative factor used in the update rules. One algorithm can be 
shown to minimize the conventional least squares error while the other 
minimizes the generalized Kullback-Leibler divergence. The monotonic 
convergence of both algorithms can be proven using an auxiliary func- 
tion analogous to that used for proving convergence of the Expectation- 
Maximization algorithm. The algorithms can also be interpreted as diag- 
onally rescaled gradient descent, where the rescaling factor is optimally 
chosen to ensure convergence.},
	urldate = {2023-03-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Lee, Daniel and Seung, H. Sebastian},
	year = {2000},
}

@misc{webersinke_climatebert_2022,
	title = {{ClimateBert}: {A} {Pretrained} {Language} {Model} for {Climate}-{Related} {Text}},
	shorttitle = {{ClimateBert}},
	url = {http://arxiv.org/abs/2110.12010},
	doi = {10.48550/arXiv.2110.12010},
	abstract = {Over the recent years, large pretrained language models (LM) have revolutionized the field of natural language processing (NLP). However, while pretraining on general language has been shown to work very well for common language, it has been observed that niche language poses problems. In particular, climate-related texts include specific language that common LMs can not represent accurately. We argue that this shortcoming of today's LMs limits the applicability of modern NLP to the broad field of text processing of climate-related texts. As a remedy, we propose CLIMATEBERT, a transformer-based language model that is further pretrained on over 2 million paragraphs of climate-related texts, crawled from various sources such as common news, research articles, and climate reporting of companies. We find that CLIMATEBERT leads to a 48\% improvement on a masked language model objective which, in turn, leads to lowering error rates by 3.57\% to 35.71\% for various climate-related downstream tasks like text classification, sentiment analysis, and fact-checking.},
	urldate = {2023-03-13},
	publisher = {arXiv},
	author = {Webersinke, Nicolas and Kraus, Mathias and Bingler, Julia Anna and Leippold, Markus},
	month = dec,
	year = {2022},
	note = {arXiv:2110.12010 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{varini_climatext_2021,
	title = {{ClimaText}: {A} {Dataset} for {Climate} {Change} {Topic} {Detection}},
	shorttitle = {{ClimaText}},
	url = {http://arxiv.org/abs/2012.00483},
	doi = {10.48550/arXiv.2012.00483},
	abstract = {Climate change communication in the mass media and other textual sources may affect and shape public perception. Extracting climate change information from these sources is an important task, e.g., for filtering content and e-discovery, sentiment analysis, automatic summarization, question-answering, and fact-checking. However, automating this process is a challenge, as climate change is a complex, fast-moving, and often ambiguous topic with scarce resources for popular text-based AI tasks. In this paper, we introduce {\textbackslash}textsc\{ClimaText\}, a dataset for sentence-based climate change topic detection, which we make publicly available. We explore different approaches to identify the climate change topic in various text sources. We find that popular keyword-based models are not adequate for such a complex and evolving task. Context-based algorithms like BERT {\textbackslash}cite\{devlin2018bert\} can detect, in addition to many trivial cases, a variety of complex and implicit topic patterns. Nevertheless, our analysis reveals a great potential for improvement in several directions, such as, e.g., capturing the discussion on indirect effects of climate change. Hence, we hope this work can serve as a good starting point for further research on this topic.},
	urldate = {2023-03-13},
	publisher = {arXiv},
	author = {Varini, Francesco S. and Boyd-Graber, Jordan and Ciaramita, Massimiliano and Leippold, Markus},
	month = jan,
	year = {2021},
	note = {arXiv:2012.00483 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{nguyen_bertweet_2020,
	address = {Online},
	title = {{BERTweet}: {A} pre-trained language model for {English} {Tweets}},
	shorttitle = {{BERTweet}},
	url = {https://aclanthology.org/2020.emnlp-demos.2},
	doi = {10.18653/v1/2020.emnlp-demos.2},
	abstract = {We present BERTweet, the first public large-scale pre-trained language model for English Tweets. Our BERTweet, having the same architecture as BERT-base (Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu et al., 2019). Experiments show that BERTweet outperforms strong baselines RoBERTa-base and XLM-R-base (Conneau et al., 2020), producing better performance results than the previous state-of-the-art models on three Tweet NLP tasks: Part-of-speech tagging, Named-entity recognition and text classification. We release BERTweet under the MIT License to facilitate future research and applications on Tweet data. Our BERTweet is available at https://github.com/VinAIResearch/BERTweet},
	urldate = {2023-03-08},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Nguyen, Dat Quoc and Vu, Thanh and Tuan Nguyen, Anh},
	month = oct,
	year = {2020},
	pages = {9--14},
}

@article{yi_topic_2020,
	title = {Topic {Modeling} for {Short} {Texts} via {Word} {Embedding} and {Document} {Correlation}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.2973207},
	abstract = {Topic modeling is a widely studied foundational and interesting problem in the text mining domains. Conventional topic models based on word co-occurrences infer the hidden semantic structure from a corpus of documents. However, due to the limited length of short text, data sparsity impedes the inference process of conventional topic models and causes unsatisfactory results on short texts. In fact, each short text usually contains a limited number of topics, and understanding semantic content of short text needs to the relevant background knowledge. Inspired by the observed information, we propose a regularized non-negative matrix factorization topic model for short texts, named TRNMF. The proposed model leverages pre-trained distributional vector representation of words to overcome the data sparsity problem of short texts. Meanwhile, the method employs the clustering mechanism under document-to-topic distributions during the topic inference by using Gibbs Sampling Dirichlet Multinomial Mixture model. TRNMF integrates successfully both word co-occurrence regularization and sentence similarity regularization into topic modeling for short texts. Through extensive experiments on constructed real-world short text corpus, experimental results show that TRNMF can achieve better results than the state-of-the-art methods in term of topic coherence measure and text classification task.},
	journal = {IEEE Access},
	author = {Yi, Feng and Jiang, Bo and Wu, Jianjun},
	year = {2020},
	keywords = {Computational modeling, Correlation, Data models, Knowledge based systems, Semantics, Solid modeling, Task analysis, Topic model, document correlation, non-negative matrix factorization, regularization, short texts, word embedding},
	pages = {30692--30705},
}

@incollection{kuang_nonnegative_2015,
	address = {Cham},
	title = {Nonnegative {Matrix} {Factorization} for {Interactive} {Topic} {Modeling} and {Document} {Clustering}},
	isbn = {9783319092591},
	url = {https://doi.org/10.1007/978-3-319-09259-1_7},
	abstract = {Nonnegative matrix factorization (NMF) approximates a nonnegative matrix by the product of two low-rank nonnegative matrices. Since it gives semantically meaningful result that is easily interpretable in clustering applications, NMF has been widely used as a clustering method especially for document data, and as a topic modeling method.We describe several fundamental facts of NMF and introduce its optimization framework called block coordinate descent. In the context of clustering, our framework provides a flexible way to extend NMF such as the sparse NMF and the weakly-supervised NMF. The former provides succinct representations for better interpretations while the latter flexibly incorporate extra information and user feedback in NMF, which effectively works as the basis for the visual analytic topic modeling system that we present.Using real-world text data sets, we present quantitative experimental results showing the superiority of our framework from the following aspects: fast convergence, high clustering accuracy, sparse representation, consistent output, and user interactivity. In addition, we present a visual analytic system called UTOPIAN (User-driven Topic modeling based on Interactive NMF) and show several usage scenarios.Overall, our book chapter cover the broad spectrum of NMF in the context of clustering and topic modeling, from fundamental algorithmic behaviors to practical visual analytics systems.},
	language = {en},
	urldate = {2023-03-07},
	booktitle = {Partitional {Clustering} {Algorithms}},
	publisher = {Springer International Publishing},
	author = {Kuang, Da and Choo, Jaegul and Park, Haesun},
	editor = {Celebi, M. Emre},
	year = {2015},
	doi = {10.1007/978-3-319-09259-1_7},
	keywords = {Block coordinate descent, Document clustering, Interactive visual analytics, Nonnegative matrix factorization, Topic modeling},
	pages = {215--243},
}

@article{yin_dirichlet_2014,
	title = {A dirichlet multinomial mixture model-based approach for short text clustering},
	url = {https://dl.acm.org/doi/10.1145/2623330.2623715},
	doi = {10.1145/2623330.2623715},
	abstract = {Short text clustering has become an increasingly important task with the popularity of social media like Twitter, Google+, and Facebook. It is a challenging problem due to its sparse, high-dimensional, and large-volume characteristics. In this paper, we proposed a collapsed Gibbs Sampling algorithm for the Dirichlet Multinomial Mixture model for short text clustering (abbr. to GSDMM). We found that GSDMM can infer the number of clusters automatically with a good balance between the completeness and homogeneity of the clustering results, and is fast to converge. GSDMM can also cope with the sparse and high-dimensional problem of short texts, and can obtain the representative words of each cluster. Our extensive experimental study shows that GSDMM can achieve significantly better performance than three other clustering models.},
	language = {en},
	urldate = {2023-03-03},
	journal = {Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining},
	author = {Yin, Jianhua and Wang, Jianyong},
	month = aug,
	year = {2014},
	pages = {233--242},
}

@misc{feng_context_2020,
	title = {Context {Reinforced} {Neural} {Topic} {Modeling} over {Short} {Texts}},
	url = {http://arxiv.org/abs/2008.04545},
	doi = {10.48550/arXiv.2008.04545},
	abstract = {As one of the prevalent topic mining tools, neural topic modeling has attracted a lot of interests for the advantages of high efficiency in training and strong generalisation abilities. However, due to the lack of context in each short text, the existing neural topic models may suffer from feature sparsity on such documents. To alleviate this issue, we propose a Context Reinforced Neural Topic Model (CRNTM), whose characteristics can be summarized as follows. Firstly, by assuming that each short text covers only a few salient topics, CRNTM infers the topic for each word in a narrow range. Secondly, our model exploits pre-trained word embeddings by treating topics as multivariate Gaussian distributions or Gaussian mixture distributions in the embedding space. Extensive experiments on two benchmark datasets validate the effectiveness of the proposed model on both topic discovery and text classification.},
	urldate = {2023-03-03},
	publisher = {arXiv},
	author = {Feng, Jiachun and Zhang, Zusheng and Ding, Cheng and Rao, Yanghui and Xie, Haoran},
	month = aug,
	year = {2020},
	note = {arXiv:2008.04545 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
}

@misc{angelov_top2vec_2020,
	title = {{Top2Vec}: {Distributed} {Representations} of {Topics}},
	shorttitle = {{Top2Vec}},
	url = {http://arxiv.org/abs/2008.09470},
	doi = {10.48550/arXiv.2008.09470},
	abstract = {Topic modeling is used for discovering latent semantic structure, usually referred to as topics, in a large collection of documents. The most widely used methods are Latent Dirichlet Allocation and Probabilistic Latent Semantic Analysis. Despite their popularity they have several weaknesses. In order to achieve optimal results they often require the number of topics to be known, custom stop-word lists, stemming, and lemmatization. Additionally these methods rely on bag-of-words representation of documents which ignore the ordering and semantics of words. Distributed representations of documents and words have gained popularity due to their ability to capture semantics of words and documents. We present \${\textbackslash}texttt\{top2vec\}\$, which leverages joint document and word semantic embedding to find \${\textbackslash}textit\{topic vectors\}\$. This model does not require stop-word lists, stemming or lemmatization, and it automatically finds the number of topics. The resulting topic vectors are jointly embedded with the document and word vectors with distance between them representing semantic similarity. Our experiments demonstrate that \${\textbackslash}texttt\{top2vec\}\$ finds topics which are significantly more informative and representative of the corpus trained on than probabilistic generative models.},
	urldate = {2023-03-03},
	publisher = {arXiv},
	author = {Angelov, Dimo},
	month = aug,
	year = {2020},
	note = {arXiv:2008.09470 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{grootendorst_bertopic_2022,
	title = {{BERTopic}: {Neural} topic modeling with a class-based {TF}-{IDF} procedure},
	shorttitle = {{BERTopic}},
	url = {http://arxiv.org/abs/2203.05794},
	doi = {10.48550/arXiv.2203.05794},
	abstract = {Topic models can be useful tools to discover latent topics in collections of documents. Recent studies have shown the feasibility of approach topic modeling as a clustering task. We present BERTopic, a topic model that extends this process by extracting coherent topic representation through the development of a class-based variation of TF-IDF. More specifically, BERTopic generates document embedding with pre-trained transformer-based language models, clusters these embeddings, and finally, generates topic representations with the class-based TF-IDF procedure. BERTopic generates coherent topics and remains competitive across a variety of benchmarks involving classical models and those that follow the more recent clustering approach of topic modeling.},
	urldate = {2023-03-03},
	publisher = {arXiv},
	author = {Grootendorst, Maarten},
	month = mar,
	year = {2022},
	note = {arXiv:2203.05794 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{murshed_short_2022,
	title = {Short text topic modelling approaches in the context of big data: taxonomy, survey, and analysis},
	issn = {1573-7462},
	shorttitle = {Short text topic modelling approaches in the context of big data},
	url = {https://doi.org/10.1007/s10462-022-10254-w},
	doi = {10.1007/s10462-022-10254-w},
	abstract = {Social media platforms such as (Twitter, Facebook, and Weibo) are being increasingly embraced by individuals, groups, and organizations as a valuable source of information. This social media generated information comes in the form of tweets or posts, and normally characterized as short text, huge, sparse, and low density. Since many real-world applications need semantic interpretation of such short texts, research in Short Text Topic Modeling (STTM) has recently gained a lot of interest to reveal unique and cohesive latent topics. This article examines the current state of the art in STTM algorithms. It presents a comprehensive survey and taxonomy of STTM algorithms for short text topic modelling. The article also includes a qualitative and quantitative study of the STTM algorithms, as well as analyses of the various strengths and drawbacks of STTM techniques. Moreover, a comparative analysis of the topic quality and performance of representative STTM models is presented. The performance evaluation is conducted on two real-world Twitter datasets: the Real-World Pandemic Twitter (RW-Pand-Twitter) dataset and Real-world Cyberbullying Twitter (RW-CB-Twitter) dataset in terms of several metrics such as topic coherence, purity, NMI, and accuracy. Finally, the open challenges and future research directions in this promising field are discussed to highlight the trends of research in STTM. The work presented in this paper is useful for researchers interested in learning state-of-the-art short text topic modelling and researchers focusing on developing new algorithms for short text topic modelling.},
	language = {en},
	urldate = {2023-03-02},
	journal = {Artificial Intelligence Review},
	author = {Murshed, Belal Abdullah Hezam and Mallappa, Suresha and Abawajy, Jemal and Saif, Mufeed Ahmed Naji and Al-ariki, Hasib Daowd Esmail and Abdulwahab, Hudhaifa Mohammed},
	month = oct,
	year = {2022},
	keywords = {Big data, Coherence, Data streaming, Deep learning topic modeling, Short text topic modeling, Social media, Sparseness},
}

@article{egger_topic_2022,
	title = {A {Topic} {Modeling} {Comparison} {Between} {LDA}, {NMF}, {Top2Vec}, and {BERTopic} to {Demystify} {Twitter} {Posts}},
	volume = {7},
	issn = {2297-7775},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9120935/},
	doi = {10.3389/fsoc.2022.886498},
	abstract = {The richness of social media data has opened a new avenue for social science research to gain insights into human behaviors and experiences. In particular, emerging data-driven approaches relying on topic models provide entirely new perspectives on interpreting social phenomena. However, the short, text-heavy, and unstructured nature of social media content often leads to methodological challenges in both data collection and analysis. In order to bridge the developing field of computational science and empirical social research, this study aims to evaluate the performance of four topic modeling techniques; namely latent Dirichlet allocation (LDA), non-negative matrix factorization (NMF), Top2Vec, and BERTopic. In view of the interplay between human relations and digital media, this research takes Twitter posts as the reference point and assesses the performance of different algorithms concerning their strengths and weaknesses in a social science context. Based on certain details during the analytical procedures and on quality issues, this research sheds light on the efficacy of using BERTopic and NMF to analyze Twitter data.},
	urldate = {2023-03-02},
	journal = {Frontiers in Sociology},
	author = {Egger, Roman and Yu, Joanne},
	month = may,
	year = {2022},
	pmid = {35602001},
	pmcid = {PMC9120935},
	pages = {886498},
}

@article{roberts_structural_nodate,
	title = {The {Structural} {Topic} {Model} and {Applied} {Social} {Science}},
	abstract = {We develop the Structural Topic Model which provides a general way to incorporate corpus structure or document metadata into the standard topic model. Document-level covariates enter the model through a simple generalized linear model framework in the prior distributions controlling either topical prevalence or topical content. We demonstrate the model’s use in two applied problems: the analysis of open-ended responses in a survey experiment about immigration policy, and understanding differing media coverage of China’s rise.},
	language = {en},
	author = {Roberts, Margaret E and Tingley, Dustin and Stewart, Brandon M and Airoldi, Edoardo M},
	pages = {4},
}

@article{blei_latent_2003,
	title = {Latent dirichlet allocation},
	volume = {3},
	issn = {1532-4435},
	abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
	number = {null},
	journal = {The Journal of Machine Learning Research},
	author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
	month = mar,
	year = {2003},
	pages = {993--1022},
}

@inproceedings{yan_biterm_2013,
	address = {New York, NY, USA},
	series = {{WWW} '13},
	title = {A biterm topic model for short texts},
	isbn = {978-1-4503-2035-1},
	url = {https://doi.org/10.1145/2488388.2488514},
	doi = {10.1145/2488388.2488514},
	abstract = {Uncovering the topics within short texts, such as tweets and instant messages, has become an important task for many content analysis applications. However, directly applying conventional topic models (e.g. LDA and PLSA) on such short texts may not work well. The fundamental reason lies in that conventional topic models implicitly capture the document-level word co-occurrence patterns to reveal topics, and thus suffer from the severe data sparsity in short documents. In this paper, we propose a novel way for modeling topics in short texts, referred as biterm topic model (BTM). Specifically, in BTM we learn the topics by directly modeling the generation of word co-occurrence patterns (i.e. biterms) in the whole corpus. The major advantages of BTM are that 1) BTM explicitly models the word co-occurrence patterns to enhance the topic learning; and 2) BTM uses the aggregated patterns in the whole corpus for learning topics to solve the problem of sparse word co-occurrence patterns at document-level. We carry out extensive experiments on real-world short text collections. The results demonstrate that our approach can discover more prominent and coherent topics, and significantly outperform baseline methods on several evaluation metrics. Furthermore, we find that BTM can outperform LDA even on normal texts, showing the potential generality and wider usage of the new topic model.},
	urldate = {2022-08-15},
	booktitle = {Proceedings of the 22nd international conference on {World} {Wide} {Web}},
	publisher = {Association for Computing Machinery},
	author = {Yan, Xiaohui and Guo, Jiafeng and Lan, Yanyan and Cheng, Xueqi},
	month = may,
	year = {2013},
	keywords = {biterm, content analysis, short text, topic model},
	pages = {1445--1456},
}

@misc{vaswani_attention_2017,
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	number = {{arXiv}:1706.03762},
	publisher = {{arXiv}},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2023-06-20},
	date = {2017-12-05},
	eprinttype = {arxiv},
	eprint = {1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}


@article{mcinnes_hdbscan_2017,
	title = {hdbscan: Hierarchical density based clustering},
	volume = {2},
	issn = {2475-9066},
	url = {https://joss.theoj.org/papers/10.21105/joss.00205},
	doi = {10.21105/joss.00205},
	shorttitle = {hdbscan},
	abstract = {{McInnes} et al, (2017), hdbscan: Hierarchical density based clustering, Journal of Open Source Software, 2(11), 205, doi:10.21105/joss.00205},
	pages = {205},
	number = {11},
	journaltitle = {Journal of Open Source Software},
	author = {{McInnes}, Leland and Healy, John and Astels, Steve},
	urldate = {2023-06-20},
	date = {2017-03-21},
	langid = {english},
}

@inproceedings{lu_RIBS_2017,
	title = {Don't Forget the Quantifiable Relationship between Words: Using Recurrent Neural Network for Short Text Topic Discovery},
	volume = {31},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/10670},
	doi = {10.1609/aaai.v31i1.10670},
	shorttitle = {Don't Forget the Quantifiable Relationship between Words},
	abstract = {In our daily life, short texts have been everywhere especially since the emergence of social network. There are countless short texts in online media like twitter, online Q\&A sites and so on. Discovering topics is quite valuable in various application domains such as content recommendation and text characterization. Traditional topic models like {LDA} are widely applied for sorts of tasks, but when it comes to short text scenario, these models may get stuck due to the lack of words. Recently, a popular model named {BTM} uses word co-occurrence relationship to solve the sparsity problem and is proved effectively. However, both {BTM} and extended models ignore the inside relationship between words. From our perspectives, more related words should appear in the same topic. Based on this idea, we propose a model named {RIBS}-{TM} which makes use of {RNN} for relationship learning and {IDF} for filtering high-frequency words. Experiments on two real-world short text datasets show great utility of our model.},
	booktitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	author = {Lu, Heng-Yang and Xie, Lu-Yao and Kang, Ning and Wang, Chong-Jun and Xie, Jun-Yuan},
	urldate = {2023-06-20},
	date = {2017-02-12},
}

@inproceedings{terragni2020octis,
    title={{OCTIS}: Comparing and Optimizing Topic Models is Simple!},
    author={Terragni, Silvia and Fersini, Elisabetta and Galuzzi, Bruno Giovanni and Tropeano, Pietro and Candelieri, Antonio},
    year={2021},
    booktitle={Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations},
    month = apr,
    year = "2021",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2021.eacl-demos.31",
    pages = "263--270",
}

@inproceedings{DBLP:conf/clic-it/TerragniF21,
  author    = {Silvia Terragni and Elisabetta Fersini},
  editor    = {Elisabetta Fersini and Marco Passarotti and Viviana Patti},
  title     = {{OCTIS 2.0: Optimizing and Comparing Topic Models in Italian Is Even
               Simpler!}},
  booktitle = {Proceedings of the Eighth Italian Conference on Computational Linguistics,
               CLiC-it 2021, Milan, Italy, January 26-28, 2022},
  series    = {{CEUR} Workshop Proceedings},
  volume    = {3033},
  publisher = {CEUR-WS.org},
  year      = {2021},
  url       = {http://ceur-ws.org/Vol-3033/paper55.pdf},
}

@article{vega_foundations_2018,
	title = {Foundations of {Temporal} {Text} {Networks}},
	volume = {3},
	copyright = {2018 The Author(s)},
	issn = {2364-8228},
	url = {https://appliednetsci.springeropen.com/articles/10.1007/s41109-018-0082-3},
	doi = {10.1007/s41109-018-0082-3},
	abstract = {Three fundamental elements to understand human information networks are the individuals (actors) in the network, the information they exchange, that is often observable online as text content (emails, social media posts, etc.), and the time when these exchanges happen. An extremely large amount of research has addressed some of these aspects either in isolation or as combinations of two of them. There are also more and more works studying systems where all three elements are present, but typically using ad hoc models and algorithms that cannot be easily transfered to other contexts. To address this heterogeneity, in this article we present a simple, expressive and extensible model for temporal text networks, that we claim can be used as a common ground across different types of networks and analysis tasks, and we show how simple procedures to produce views of the model allow the direct application of analysis methods already developed in other domains, from traditional data mining to multilayer network mining.},
	language = {en},
	number = {1},
	urldate = {2023-06-22},
	journal = {Applied Network Science},
	author = {Vega, Davide and Magnani, Matteo},
	month = dec,
	year = {2018},
	pages = {1--26},
}

@article{McInnes2018, doi = {10.21105/joss.00861}, url = {https://doi.org/10.21105/joss.00861}, year = {2018}, publisher = {The Open Journal}, volume = {3}, number = {29}, pages = {861}, author = {Leland McInnes and John Healy and Nathaniel Saul and Lukas Großberger}, title = {UMAP: Uniform Manifold Approximation and Projection}, journal = {Journal of Open Source Software} }

@misc{cer_use_2018,
	title = {Universal Sentence Encoder},
	url = {http://arxiv.org/abs/1803.11175},
	doi = {10.48550/arXiv.1803.11175},
	abstract = {We present models for encoding sentences into embedding vectors that specifically target transfer learning to other {NLP} tasks. The models are efficient and result in accurate performance on diverse transfer tasks. Two variants of the encoding models allow for trade-offs between accuracy and compute resources. For both variants, we investigate and report the relationship between model complexity, resource consumption, the availability of transfer task training data, and task performance. Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning. We find that transfer learning using sentence embeddings tends to outperform word level transfer. With transfer learning via sentence embeddings, we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task. We obtain encouraging results on Word Embedding Association Tests ({WEAT}) targeted at detecting model bias. Our pre-trained sentence encoding models are made freely available for download and on {TF} Hub.},
	number = {{arXiv}:1803.11175},
	publisher = {{arXiv}},
	author = {Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole and John, Rhomni St and Constant, Noah and Guajardo-Cespedes, Mario and Yuan, Steve and Tar, Chris and Sung, Yun-Hsuan and Strope, Brian and Kurzweil, Ray},
	urldate = {2023-07-12},
	date = {2018-04-12},
	eprinttype = {arxiv},
	eprint = {1803.11175 [cs]},
	keywords = {Computer Science - Computation and Language},
}


@article{hartigan_dip_1985,
	title = {The {Dip} {Test} of {Unimodality}},
	volume = {13},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-13/issue-1/The-Dip-Test-of-Unimodality/10.1214/aos/1176346577.full},
	doi = {10.1214/aos/1176346577},
	abstract = {The dip test measures multimodality in a sample by the maximum difference, over all sample points, between the empirical distribution function, and the unimodal distribution function that minimizes that maximum difference. The uniform distribution is the asymptotically least favorable unimodal distribution, and the distribution of the test statistic is determined asymptotically and empirically when sampling from the uniform.},
	number = {1},
	urldate = {2023-09-06},
	journal = {The Annals of Statistics},
	author = {Hartigan, J. A. and Hartigan, P. M.},
	month = mar,
	year = {1985},
	keywords = {62F05, 62G05, Empirical distribution, isotonic regression, multimodality},
	pages = {70--84},
}

@article{flamino_shifting_2021,
	title = {Shifting {Polarization} and {Twitter} {News} {Influencers} between two {U}.{S}. {Presidential} {Elections}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2111.02505},
	doi = {10.48550/ARXIV.2111.02505},
	abstract = {Social media are decentralized, interactive, and transformative, empowering users to produce and spread information to influence others. This has changed the dynamics of political communication that were previously dominated by traditional corporate news media. Having hundreds of millions of tweets collected over the 2016 and 2020 U.S. presidential elections gave us a unique opportunity to measure the change in polarization and the diffusion of political information. We analyze the diffusion of political information among Twitter users and investigate the change of polarization between these elections and how this change affected the composition and polarization of influencers and their retweeters. We identify "influencers" by their ability to spread information and classify them into those affiliated with a media organization, a political organization, or unaffiliated. Most of the top influencers were affiliated with media organizations during both elections. We found a clear increase from 2016 to 2020 in polarization among influencers and among those whom they influence. Moreover, 75\% of the top influencers in 2020 were not present in 2016, demonstrating that such status is difficult to retain. Between 2016 and 2020, 10\% of influencers affiliated with media were replaced by center- or right-orientated influencers affiliated with political organizations and unaffiliated influencers.},
	urldate = {2023-09-06},
	author = {Flamino, James and Galezzi, Alessandro and Feldman, Stuart and Macy, Michael W. and Cross, Brendan and Zhou, Zhenkun and Serafino, Matteo and Bovet, Alexandre and Makse, Hernan A. and Szymanski, Boleslaw K.},
	year = {2021},
	keywords = {Computers and Society (cs.CY), FOS: Computer and information sciences, Social and Information Networks (cs.SI)},
}

@article{barbera_tweeting_2015,
	title = {Tweeting {From} {Left} to {Right}: {Is} {Online} {Political} {Communication} {More} {Than} an {Echo} {Chamber}?},
	volume = {26},
	issn = {0956-7976, 1467-9280},
	shorttitle = {Tweeting {From} {Left} to {Right}},
	url = {http://journals.sagepub.com/doi/10.1177/0956797615594620},
	doi = {10.1177/0956797615594620},
	abstract = {We estimated ideological preferences of 3.8 million Twitter users and, using a data set of nearly 150 million tweets concerning 12 political and nonpolitical issues, explored whether online communication resembles an “echo chamber” (as a result of selective exposure and ideological segregation) or a “national conversation.” We observed that information was exchanged primarily among individuals with similar ideological preferences in the case of political issues (e.g., 2012 presidential election, 2013 government shutdown) but not many other current events (e.g., 2013 Boston Marathon bombing, 2014 Super Bowl). Discussion of the Newtown shootings in 2012 reflected a dynamic process, beginning as a national conversation before transforming into a polarized exchange. With respect to both political and nonpolitical issues, liberals were more likely than conservatives to engage in cross-ideological dissemination; this is an important asymmetry with respect to the structure of communication that is consistent with psychological theory and research bearing on ideological differences in epistemic, existential, and relational motivation. Overall, we conclude that previous work may have overestimated the degree of ideological segregation in social-media usage.},
	language = {en},
	number = {10},
	urldate = {2023-09-06},
	journal = {Psychological Science},
	author = {Barberá, Pablo and Jost, John T. and Nagler, Jonathan and Tucker, Joshua A. and Bonneau, Richard},
	month = oct,
	year = {2015},
	pages = {1531--1542},
}

@article{barbera_birds_2015,
	title = {Birds of the {Same} {Feather} {Tweet} {Together}: {Bayesian} {Ideal} {Point} {Estimation} {Using} {Twitter} {Data}},
	volume = {23},
	issn = {1047-1987, 1476-4989},
	shorttitle = {Birds of the {Same} {Feather} {Tweet} {Together}},
	url = {https://www.cambridge.org/core/journals/political-analysis/article/birds-of-the-same-feather-tweet-together-bayesian-ideal-point-estimation-using-twitter-data/91E37205F69AEA32EF27F12563DC2A0A},
	doi = {10.1093/pan/mpu011},
	abstract = {Politicians and citizens increasingly engage in political conversations on social media outlets such as Twitter. In this article, I show that the structure of the social networks in which they are embedded can be a source of information about their ideological positions. Under the assumption that social networks are homophilic, I develop a Bayesian Spatial Following model that considers ideology as a latent variable, whose value can be inferred by examining which politics actors each user is following. This method allows us to estimate ideology for more actors than any existing alternative, at any point in time and across many polities. I apply this method to estimate ideal points for a large sample of both elite and mass public Twitter users in the United States and five European countries. The estimated positions of legislators and political parties replicate conventional measures of ideology. The method is also able to successfully classify individuals who state their political preferences publicly and a sample of users matched with their party registration records. To illustrate the potential contribution of these estimates, I examine the extent to which online behavior during the 2012 US presidential election campaign is clustered along ideological lines.},
	language = {en},
	number = {1},
	urldate = {2023-09-06},
	journal = {Political Analysis},
	author = {Barberá, Pablo},
	month = jan,
	year = {2015},
	pages = {76--91},
}

@article{vega_foundations_2018,
	title = {Foundations of {Temporal} {Text} {Networks}},
	volume = {3},
	copyright = {2018 The Author(s)},
	issn = {2364-8228},
	url = {https://appliednetsci.springeropen.com/articles/10.1007/s41109-018-0082-3},
	doi = {10.1007/s41109-018-0082-3},
	abstract = {Three fundamental elements to understand human information networks are the individuals (actors) in the network, the information they exchange, that is often observable online as text content (emails, social media posts, etc.), and the time when these exchanges happen. An extremely large amount of research has addressed some of these aspects either in isolation or as combinations of two of them. There are also more and more works studying systems where all three elements are present, but typically using ad hoc models and algorithms that cannot be easily transfered to other contexts. To address this heterogeneity, in this article we present a simple, expressive and extensible model for temporal text networks, that we claim can be used as a common ground across different types of networks and analysis tasks, and we show how simple procedures to produce views of the model allow the direct application of analysis methods already developed in other domains, from traditional data mining to multilayer network mining.},
	language = {en},
	number = {1},
	urldate = {2023-06-22},
	journal = {Applied Network Science},
	author = {Vega, Davide and Magnani, Matteo},
	month = dec,
	year = {2018},
	pages = {1--26},
}

@article{falkenberg_growing_2022,
	title = {Growing polarization around climate change on social media},
	volume = {12},
	copyright = {2022 The Author(s)},
	issn = {1758-6798},
	url = {https://www.nature.com/articles/s41558-022-01527-x},
	doi = {10.1038/s41558-022-01527-x},
	abstract = {Climate change and political polarization are two of the twenty-first century’s critical socio-political issues. Here we investigate their intersection by studying the discussion around the United Nations Conference of the Parties on Climate Change (COP) using Twitter data from 2014 to 2021. First, we reveal a large increase in ideological polarization during COP26, following low polarization between COP20 and COP25. Second, we show that this increase is driven by growing right-wing activity, a fourfold increase since COP21 relative to pro-climate groups. Finally, we identify a broad range of ‘climate contrarian’ views during COP26, emphasizing the theme of political hypocrisy as a topic of cross-ideological appeal; contrarian views and accusations of hypocrisy have become key themes in the Twitter climate discussion since 2019. With future climate action reliant on negotiations at COP27 and beyond, our results highlight the importance of monitoring polarization and its impacts in the public climate discourse.},
	language = {en},
	number = {12},
	urldate = {2023-03-29},
	journal = {Nature Climate Change},
	author = {Falkenberg, Max and Galeazzi, Alessandro and Torricelli, Maddalena and Di Marco, Niccolò and Larosa, Francesca and Sas, Madalina and Mekacher, Amin and Pearce, Warren and Zollo, Fabiana and Quattrociocchi, Walter and Baronchelli, Andrea},
	month = dec,
	year = {2022},
	keywords = {Climate change, Communication, Interdisciplinary studies, Mathematics and computing},
	pages = {1114--1121},
}
