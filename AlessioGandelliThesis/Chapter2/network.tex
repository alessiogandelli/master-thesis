\section[Networks]{Networks}

A network, often referred to as a graph, is a data structure that is composed of nodes and ties, which represent the connections between the nodes. This interconnected structure allows for the representation and analysis of relationships and dependencies among various entities. The applications of networks are incredibly diverse and wide-ranging.

One example of a network is a computer network, which consists of interconnected computers that communicate and share resources. Another example is a network of webpages, where each webpage is represented as a node, and the links between webpages form the ties. This type of network is fundamental to the World Wide Web and enables the navigation and discovery of information online. Additionally, networks find applications in logistics and transportation, where they are utilized to optimize routes and streamline the movement of goods and service. These are only few examples in the use of networks, in this thesis the focus will be on social networks, where the nodes represents persons and the tie some sort of interaction between them.


Social Network Analysis is a field that examines the relationships, the interactions, and the structures within a network of individuals or entities. It provides valuable insights into the dynamics, information flow, and influence within social networks. Several studies have
 applied SNA in different domains \cite{borgatti_network_2009}, such as online social networks, organizational networks, and public health networks. A growing field in this context is the analysis of social media, which completely changed how the research in this area is carried out. Due to the huge amount of data available, it is possible to shift to a data driven methodology, where the data collection is not strictly embedded in the design of the experiment, but it is the starting point on top of which the research is built. Another advantage of this kind of data is that, since it results from a natural observation of actors in a social environment, it is not as biased as self-reported data or as data collected in a laboratory setting. Concerning this, Veltri \cite{digital_veltri_2019} explains how behavioral data is linked to automatic decisions in the surrounding environments, while the self reported one is more conscious and reflexive and not always the action matches what people say. It must be noted that the data is not completely unbiased, it depends on the goal and the structure that the platform gave to it.

In this scenario, the data is being used to explore the topology of the interaction between users of Twitter; the structure of their interactions is studied to understand if there are some recurring patterns.

\subsection{Multi-layer Networks}
In particular, we will build our network using a Multi-layer networks (MLN) framework\cite{kivela_multilayer_2014}. MLN are complex networks that capture multiple types of relationships, or interactions, between nodes. They allow for the representation of different dimensions or contexts in a single framework, providing a more comprehensive understanding of network dynamics. Single-layer networks are, in certain cases, an oversimplification of reality\cite{trax_multilayer_16} \cite{hammoud_multilayer_2020}.

MLN are often used in biological networks where, due to organism complexity, every biological function is usually influenced by more factors, and modeling with a MLN helps the researcher in studying the interaction between these factors. Another biological use of MLN is epidemiology, where the presence of a certain disease can be strongly influenced by other clinical conditions. For example Kinsley et al. \cite{kinsley_multilayer_2020} used this framework in veterinary epidemiology to identify the subjects that are more prone to spread a certain disease.

Also in the field of interest for this research, social networks and, in particular, Twitter, several researchers used MLN to structure their study: for instance ref \cite{nguyen_twitter_21} employed it to identify the most central accounts over multiple layers in the discussion of different political candidates, one for each layer. In relation to this, De Domenico \cite{de_domenico_ranking_2015} mathematically described different ways to compute node centrality on multiple layers, introducing the concept of versatility.

Thanks to the complexity added by the multiple layers, the same users can be seen interacting in the different dimensions, which in our case are the different topics. This implies that for each identified topic In the topic modeling phase, it is possible to observe a network of interactions among users, allowing the study of the users’ presence on multiple topics.




\section[Polarization]{Polarization}
As anticipated in chapter \ref{Introduction}, it is fundamental for the understanding of this work to comprehend what Falkenberg did in his research \cite{falkenberg_growing_2022}. Since the same methods are employed here, we also make the same assumption of the bipolarity of the polarization. In this section, it will be shown  how polarization will be computed.

A meaningful metric that gained popularity among social scientists is polarization. The researchers believe that polarization can be harmful for the maintenance of the democratic stability \cite{mccoy_polarization_democracy_2018}. Thus, understanding the phenomenon is important to develop a solution to it.

Polarization has been used to study the impact of political discussion on social media, especially around US presidential elections  \cite{conover_political_2011}
\cite{flamino_shifting_2021} . Due to this, attention should be paid when generalizing, especially since US politics are built around two main parties -Democrats and Republicans-, making a bimodal view of polarization the best suited for this case (but not for all). Despite this, an analysis of the polarization over 21 different countries shows that the US is not the only place where it has been detected  \cite{gidron_toward_2019}. Ref
\cite{radicioni_networked_2021}
show the polarization can have different geometries than the traditional bimodality.

Even though Falkenberg detected an increasing polarization only in the COP26, while in 2015, Williams et al. \cite{williams_network_2015} found the presence of echo chambers around the climate discussion in social media, with a small presence of open forums. In this work, there will be an attempt to connect these two pieces of research, to understand if, by breaking down the discussion into topics, the polarization of specific topics has always been high.

There is not a clear and universally adopted definition of polarization; Bramson et al. \cite{bramson_understanding_2017} tried to summarize it by defining different types of it; the assumption is that there is a measure of ’opinion’ for each user: 
\begin{itemize}
    \item \textbf{Spread} defines the distance between the two extremes as the breadth of opinion
    \item \textbf{Dispersion} considers the shape of the distribution of the opinions and searches for peaks
    \item \textbf{Coverage} does not look at the shape but at the similarity of opinions within the groups
    \item \textbf{Regionalization} looks at the spectrum not covered between the groups
    \item \textbf{Community fractioning} is the degree to which the population can be broken into subpopulations
    \item \textbf{Distinctness} is defined as  "the degree to which the group distribution can be separated"
    \item \textbf{Group divergence} is the opposite of distinctness, how different are the groups
    \item  \textbf{Group consensus} shoes how people in the same group have similar opinions
    \item  \textbf{Size parity } put relevance on the size of each group
\end{itemize}



We use the definition of Dispersion polarization that looks into the distribution of beliefs to detect peaks. Falkenberg demonstrated that the assumption of bimodality makes sense when dividing the population into climate supporters and climate skeptics. There is also the assumption  that  the polarization can be detected using the retweet network.

\paragraph{Latent ideology}
In order to compute polarization on a retweet network, firstly,  a latent ideology for each user is to be estimated, as defined in \cite{barbera_birds_2015} and adapted for retweets in \cite{flamino_shifting_2021}.


Starting from the adjacency matrix of the retweet network, and after some linear algebra, a latent ideology score for each user can be obtained.

The first step is to identify \textit{m }most retweeted users -from now on \textit{influencers- }, out of the \textit{n }users, and then build an adjacency matrix $A\in \mathbb{R}^{n x m}$ between users and influencers (where \(a_{ij}\) is the number of times user $i$ retweeted influencer $j$).  Now the process will be shown in detail from the matrix to the scores. In this way all the users that did not interact with the top $m$ influencers will be excluded from the analysis.

\\

Firstly, normalize $A$ by the number of retweets:
\begin{equation}
P =  \frac{A_{ij}}{\sum_{i} \sum_{j} a_{ij}}
\end{equation}

Secondly, get the vector of row and column sums and consider the diagonal matrix:
\begin{equation}
\textbf{r} \in \mathbb{R}^m, \quad  r_i = \sum_{i} a_{ij}
\end{equation}
\begin{equation}
\textbf{c} \in \mathbb{R}^n , \quad c_j = \sum_{j} a_{ij} 
\end{equation}





\begin{equation}
  R =
  \begin{bmatrix}
    \frac{1}{{\sqrt{r_{1}}}} & & \\
    & \ddots & \\
    & & \frac{1}{{\sqrt{r_{n}}}}
  \end{bmatrix}
  \;\;\;  C =
  \begin{bmatrix}
    \frac{1}{{\sqrt{c_{1}}}} & & \\
    & \ddots & \\
    & & \frac{1}{{\sqrt{c_{n}}}}
  \end{bmatrix}
\end{equation}

Then, compute the matrix of standardized residuals $S$:
\begin{equation}
S = R\left(P - (\textbf{r}\cdot \textbf{c}^T)\right)C
\end{equation}

Using Singular Value Decomposition (SVD), which is a factorization technique in linear algebra,  the standardized matrix  can be decomposed into three other matrices. It provides essential geometrical and theoretical insights about linear transformations and it is extensively used in various fields such as data science, engineering, and statistics \cite{golub1970singular}. Given  matrix $S$, its SVD is  written as:

\begin{equation}
S = U\Sigma V^T
\end{equation}

where $U$ is an $m \times m$ matrix whose columns are the orthonormal eigenvectors of $AA^T$, $\Sigma$ is an $m \times n$ diagonal matrix whose non-zero elements are the singular values of $A$, and $V^T$ is the transpose of an $n \times n$ matrix whose columns are the orthonormal eigenvectors of $A^T A$. The singular values on the diagonal of $\Sigma$ are typically sorted in descending order. The columns of $U$ and $V$ are called the left-singular vectors and right-singular vectors of $A$, respectively.
\\

Multiply $R$ and $U$:
\begin{equation}
X = R U
\end{equation}

Finally, rescale $U$ on $[-1,1]$ and get the user score:
\begin{equation}
score = -1 + 2 \cdot \frac{{X} - \min(X)}{\max(X) - \min(X)}
\end{equation}








\paragraph{Hartigan's diptest}

After computing all the users' latent ideology scores, to test the polarization,   Hartigan's diptest is used \cite{hartigan_dip_1985}.

Hartigan's Dip Test is a statistical test used to determine if a distribution is unimodal. The test works by comparing the empirical distribution function of the data, denoted as $F(x)$, to the unimodal distribution function that minimizes the maximum difference between $F(x)$ and itself, denoted as $G(x)$. The dip statistic $D$ is then defined as:

\begin{equation}
D = \sup_x |F(x) - G(x)|
\end{equation}

Where $\sup_x$ denotes the supremum (least upper bound) overall $x$, the unimodal distribution function $G(x)$ is chosen so that it minimizes this supremum. In other words, $G(x)$ is the "best" unimodal approximation to the empirical distribution function $F(x)$.

The null hypothesis of the Dip Test is that the data comes from an unimodal distribution. If the dip statistic $D$ is significantly large, the null hypothesis  is rejected and the conclusion is that the data is not unimodal. The p-value of the test is computed by comparing the observed dip statistic to the distribution of the dip statistic under the null hypothesis. This distribution is typically approximated using Monte Carlo simulations.